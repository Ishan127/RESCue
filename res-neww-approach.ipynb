{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nimport io\nimport os\nimport math\nfrom PIL import Image, ImageDraw\nimport torchvision.transforms as T\nfrom torchvision.transforms import functional as TF\nimport torch.nn.functional as F\nfrom scipy.optimize import linear_sum_assignment\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Polygon\nimport ast\nimport numpy as np\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:02.000759Z","iopub.execute_input":"2025-09-09T16:39:02.001397Z","iopub.status.idle":"2025-09-09T16:39:02.006569Z","shell.execute_reply.started":"2025-09-09T16:39:02.001367Z","shell.execute_reply":"2025-09-09T16:39:02.005855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"refcoco = load_dataset(\"Kangheng/refcoco\")  \nrefcocog = load_dataset(\"lmms-lab/RefCOCOg\")\nrefcocoplus = load_dataset(\"lmms-lab/RefCOCOplus\")\ngrefcoco = load_dataset(\"qixiangbupt/grefcoco\")\n\nprint(\"Refcoco Dataset structure:\", refcoco)\nprint(\"Available splits:\", refcoco.keys())\nprint(\"Features:\", refcoco[\"val\"].features)\n\nprint(\"\\nSome validation split samples:\")\nprint(refcoco[\"val\"][:5])\n\nprint(\"RefcocoG Dataset structure:\", refcoco)\nprint(\"Available splits:\", refcoco.keys())\nprint(\"Features:\", refcoco[\"val\"].features)\n\nprint(\"\\nSome validation split samples:\")\nprint(refcoco[\"val\"][:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:02.020139Z","iopub.execute_input":"2025-09-09T16:39:02.020478Z","iopub.status.idle":"2025-09-09T16:39:08.590010Z","shell.execute_reply.started":"2025-09-09T16:39:02.020451Z","shell.execute_reply":"2025-09-09T16:39:08.589232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_refcoco(example):\n    # Copy image\n    img = example[\"image\"].copy()\n    w, h = example[\"image_size\"]\n\n    draw = ImageDraw.Draw(img)\n\n    # Parse bounding box (string -> list of ints)\n    if \"bbox\" in example and example[\"bbox\"] is not None:\n        if isinstance(example[\"bbox\"], str):\n            bbox = ast.literal_eval(example[\"bbox\"])  # safely parse '[x1, y1, x2, y2]'\n        else:\n            bbox = example[\"bbox\"]\n\n        x1, y1, x2, y2 = bbox\n        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n\n    # Show image with title\n    plt.imshow(img)\n    plt.title(example.get(\"question\", \"\"))\n    plt.axis(\"off\")\n    plt.show()\n\ndef visualize_refcocog(example):\n    # Copy image\n    img = example[\"image\"].copy()\n    draw = ImageDraw.Draw(img)\n\n    # Bounding Box [x, y, w, h]\n    if \"bbox\" in example and example[\"bbox\"] is not None:\n        x, y, w, h = example[\"bbox\"]\n        draw.rectangle([x, y, x + w, y + h], outline=\"red\", width=3)\n\n    # Segmentation Polygon\n    if \"segmentation\" in example and example[\"segmentation\"] is not None:\n        seg = example[\"segmentation\"]\n        if isinstance(seg, list) and len(seg) > 0:\n            poly = [(seg[i], seg[i+1]) for i in range(0, len(seg), 2)]\n            draw.line(poly + [poly[0]], fill=\"blue\", width=2)\n\n    # Title: show query and first answer\n    title = example.get(\"question\", \"\")\n    if \"answer\" in example and example[\"answer\"]:\n        title += \" | Ans: \" + example[\"answer\"][0]\n\n    # Show image\n    plt.imshow(img)\n    plt.title(title, fontsize=10)\n    plt.axis(\"off\")\n    plt.show()\n\n\n\ndef visualize_refcocoplus(example):\n    \"\"\"\n    Visualize a single RefCOCO+ example with bbox, segmentation, and question/answer.\n    Args:\n        example (dict): One entry from refcocoplus[\"val\"]\n    \"\"\"\n    image = example[\"image\"]\n    question = example[\"question\"]\n    answers = example[\"answer\"]\n    bbox = example[\"bbox\"]   # [x, y, w, h]\n    seg = example[\"segmentation\"]\n\n    fig, ax = plt.subplots(1, figsize=(8, 6))\n    ax.imshow(image)\n\n    # --- Bounding Box ---\n    rect = patches.Rectangle(\n        (bbox[0], bbox[1]), bbox[2], bbox[3],\n        linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n    )\n    ax.add_patch(rect)\n\n    # --- Segmentation Polygon ---\n    if seg is not None and len(seg) > 0:\n        seg = np.array(seg).reshape(-1, 2)\n        polygon = Polygon(seg, closed=True, edgecolor=\"blue\", facecolor=\"blue\", alpha=0.3)\n        ax.add_patch(polygon)\n\n    # --- Titles ---\n    ax.set_title(f\"Q: {question}\\nAnswers: {', '.join(answers)}\", fontsize=10)\n    plt.axis(\"off\")\n    plt.show()\n\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\ndef parse_segmentation_string(seg_str):\n    \"\"\"\n    Parse a string with one or more <seg>...</seg> blocks\n    into a list of polygons (each polygon is an (N,2) numpy array).\n    \"\"\"\n    polygons = []\n    # Find all <seg>...</seg> blocks\n    seg_blocks = re.findall(r\"<seg>(.*?)</seg>\", seg_str)\n    for block in seg_blocks:\n        # Find (x,y) pairs inside this block\n        coords = re.findall(r\"\\(([\\d\\.]+),\\s*([\\d\\.]+)\\)\", block)\n        if coords:\n            poly = np.array([[float(x), float(y)] for x, y in coords], dtype=np.float32)\n            polygons.append(poly)\n    return polygons\n\ndef visualize_grefcoco_mask(example):\n    img = example['images'][0]\n    mask = example['mask_images'][0]\n\n    img_np = np.array(img)\n    mask_np = np.array(mask)\n\n    plt.figure(figsize=(20,5))\n\n    # Original Image\n    plt.subplot(1,4,1)\n    plt.imshow(img_np)\n    plt.title(\"Image\")\n    plt.axis(\"off\")\n\n    # Mask alone\n    plt.subplot(1,4,2)\n    plt.imshow(mask_np, cmap=\"gray\")\n    plt.title(\"Mask\")\n    plt.axis(\"off\")\n\n    # Image + Mask overlay\n    plt.subplot(1,4,3)\n    plt.imshow(img_np)\n    plt.imshow(mask_np, cmap=\"jet\", alpha=0.4)\n    plt.title(\"Image + Mask\")\n    plt.axis(\"off\")\n\n    # Image + Polygon overlay\n    plt.subplot(1,4,4)\n    plt.imshow(img_np)\n    if \"answer\" in example:\n        polygons = parse_segmentation_string(example[\"answer\"])\n        ax = plt.gca()\n        for poly in polygons:\n            patch = Polygon(poly, closed=True, edgecolor='lime', facecolor='none', linewidth=2)\n            ax.add_patch(patch)\n    plt.title(\"Image + Polygon(s)\")\n    plt.axis(\"off\")\n\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:08.591628Z","iopub.execute_input":"2025-09-09T16:39:08.591883Z","iopub.status.idle":"2025-09-09T16:39:08.608391Z","shell.execute_reply.started":"2025-09-09T16:39:08.591866Z","shell.execute_reply":"2025-09-09T16:39:08.607392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_refcoco(refcoco[\"val\"][0])\nvisualize_refcocog(refcocog[\"val\"][0])\nvisualize_refcocoplus(refcocoplus[\"val\"][0])\n\nexample = grefcoco['train'][0]\nvisualize_grefcoco_mask(example)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:08.609278Z","iopub.execute_input":"2025-09-09T16:39:08.609533Z","iopub.status.idle":"2025-09-09T16:39:09.842404Z","shell.execute_reply.started":"2025-09-09T16:39:08.609510Z","shell.execute_reply":"2025-09-09T16:39:09.841594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"refcoco\")\nprint(refcoco)\nprint(refcoco['val'])\nprint(refcoco['val'][0])\n\n\nprint(\"---------------------------------------\")\nprint(\"refcocoplus\")\nprint(refcocoplus)\nprint(refcocoplus['val'])\n\nprint(\"---------------------------------------\")\nprint(\"refcocog\")\nprint(refcocog)\nprint(refcocog['val'])\n\nprint(\"---------------------------------------\")\nprint(\"grefcoco\")\nprint(grefcoco)\nprint(grefcoco['train'])\nprint(grefcoco['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:09.843845Z","iopub.execute_input":"2025-09-09T16:39:09.844129Z","iopub.status.idle":"2025-09-09T16:39:09.861150Z","shell.execute_reply.started":"2025-09-09T16:39:09.844109Z","shell.execute_reply":"2025-09-09T16:39:09.860394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _pick_field(record: dict, keys: list):\n    for k in keys:\n        if k in record and record[k] is not None:\n            return record[k]\n    return None\n\ndef decode_rle_uncompressed(counts, h, w):\n    \"\"\"\n    Decode COCO-style uncompressed RLE counts (list of ints) into binary HxW mask.\n    counts: alternating counts starting with background.\n    \"\"\"\n    total = h * w\n    flat = []\n    v = 0\n    for c in counts:\n        flat.extend([v] * int(c))\n        v = 1 - v\n    flat = flat[:total]  # safety\n    arr = np.array(flat, dtype=np.uint8).reshape((h, w), order='F')  # COCO RLE uses column-major (Fortran) order\n    return Image.fromarray(arr * 255).convert('L')\n\ndef polygons_to_mask(polygons: list, h: int, w: int):\n    \"\"\"\n    polygons: list of polygon lists (x0,y0,x1,y1,...)\n    \"\"\"\n    mask = Image.new(\"L\", (w, h), 0)\n    draw = ImageDraw.Draw(mask)\n    for poly in polygons:\n        # poly might be flat list\n        try:\n            pts = [(poly[i], poly[i + 1]) for i in range(0, len(poly), 2)]\n        except Exception:\n            continue\n        draw.polygon(pts, fill=255)\n    return mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:09.862056Z","iopub.execute_input":"2025-09-09T16:39:09.862311Z","iopub.status.idle":"2025-09-09T16:39:09.869365Z","shell.execute_reply.started":"2025-09-09T16:39:09.862294Z","shell.execute_reply":"2025-09-09T16:39:09.868648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as T\nfrom PIL import Image, ImageDraw\nimport numpy as np\n\n\ndef parse_segmentation_string(seg_str):\n    \"\"\"\n    Parse one or more <seg>...</seg> blocks into polygons.\n    Returns: list of (N,2) numpy arrays\n    \"\"\"\n    polygons = []\n    if seg_str is None:\n        return polygons\n    seg_blocks = re.findall(r\"<seg>(.*?)</seg>\", seg_str)\n    for block in seg_blocks:\n        coords = re.findall(r\"\\(([\\d\\.]+),\\s*([\\d\\.]+)\\)\", block)\n        if coords:\n            poly = np.array([[float(x), float(y)] for x, y in coords], dtype=np.float32)\n            polygons.append(poly)\n    return polygons\n\n\nclass GRefCocoTorchDataset(data.Dataset):\n    def __init__(self, hf_dataset, image_size=224, train=True):\n        \"\"\"\n        Specialized dataset for gRefCOCO.\n        Expects fields: ['id', 'problem', 'answer', 'images', 'img_height', 'img_width']\n        \"\"\"\n        self.ds = hf_dataset\n        self.image_size = image_size\n        self.train = train\n\n        # Standard ImageNet transform for images\n        self.img_transform = T.Compose([\n            T.Resize((image_size, image_size)),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225])\n        ])\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        record = self.ds[idx]\n\n        # --- IMAGE ---\n        pil_img = record[\"images\"][0].convert(\"RGB\")\n        orig_w, orig_h = pil_img.size\n        img_t = self.img_transform(pil_img)  # [3,H,W]\n\n        # --- TEXT (referring expression) ---\n        txt = record.get(\"problem\", \"\")\n\n        # --- PARSE POLYGONS FROM ANSWER ---\n        seg_str = record.get(\"answer\", \"\")\n        polygons = parse_segmentation_string(seg_str)\n\n        # --- CREATE EMPTY MASK ---\n        mask_pil = Image.new(\"L\", (orig_w, orig_h), 0)\n        draw = ImageDraw.Draw(mask_pil)\n\n        # Draw polygons\n        for poly in polygons:\n            if len(poly) >= 3:  # valid polygon\n                draw.polygon([tuple(p) for p in poly], outline=1, fill=1)\n\n        # Resize to target size\n        mask_resized = mask_pil.resize((self.image_size, self.image_size), resample=Image.NEAREST)\n\n        # Convert to tensor [H,W]\n        mask_np = np.array(mask_resized, dtype=np.uint8)\n        mask_t = torch.from_numpy(mask_np).float()\n\n        # Keep consistent format [1,H,W]\n        gt_masks = mask_t.unsqueeze(0)\n\n        return {\n            \"image\": img_t,\n            \"text\": txt,\n            \"gt_masks\": gt_masks,\n            \"orig_size\": (orig_h, orig_w),\n            \"id\": record.get(\"id\", None),\n        }\n\n\n# Collate function: batch of variable number of GT masks\ndef grefcoco_collate_fn(batch):\n    images = torch.stack([b[\"image\"] for b in batch], dim=0)\n    texts = [b[\"text\"] for b in batch]\n    gt_masks_list = [b[\"gt_masks\"] for b in batch]  # each is [1,H,W]\n    return images, texts, gt_masks_list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:09.870057Z","iopub.execute_input":"2025-09-09T16:39:09.870331Z","iopub.status.idle":"2025-09-09T16:39:09.887690Z","shell.execute_reply.started":"2025-09-09T16:39:09.870308Z","shell.execute_reply":"2025-09-09T16:39:09.886974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport timm\nfrom transformers import CLIPTokenizer, CLIPTextModel\nimport warnings\nfrom typing import List, Dict\nfrom PIL import Image\nimport numpy as np\nimport json\nfrom scipy.optimize import linear_sum_assignment\nimport matplotlib.pyplot as plt\nfrom torchvision.transforms import functional as TF\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\", message=\"The `pad_to_max_length` argument is deprecated.*\", category=FutureWarning)\n\n# ======================================================================================\n# FULL MODEL ARCHITECTURE (STAGES 1, 2, 4) - UNCHANGED\n# ======================================================================================\nclass Stage1_FusionModule(nn.Module):\n    def __init__(self, hidden_dim: int = 256, vit_model_name='vit_small_patch16_224',clip_model_name='openai/clip-vit-base-patch16'):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.vit_encoder = timm.create_model(vit_model_name, pretrained=True)\n        self.text_tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n        self.text_encoder = CLIPTextModel.from_pretrained(clip_model_name)\n        self.image_projector = nn.Linear(self.vit_encoder.embed_dim, hidden_dim)\n        self.text_projector = nn.Linear(self.text_encoder.config.hidden_size, hidden_dim)\n        self.image_fusion_layers = nn.ModuleList([nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=8, dim_feedforward=hidden_dim * 4, batch_first=True) for _ in range(2)])\n        self.text_fusion_layers = nn.ModuleList([nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=8, dim_feedforward=hidden_dim * 4, batch_first=True) for _ in range(2)])\n        self.text_pos_embed = nn.Parameter(torch.randn(1, self.text_encoder.config.max_position_embeddings, hidden_dim))\n\n    def forward(self, images: torch.Tensor, texts: list[str]):\n        image_features = self.vit_encoder.forward_features(images)[:, 1:, :]\n        text_inputs = self.text_tokenizer(texts, padding='max_length', return_tensors='pt', max_length=self.text_encoder.config.max_position_embeddings).to(images.device)\n        text_features = self.text_encoder(**text_inputs).last_hidden_state\n        image_features_proj = self.image_projector(image_features)\n        text_features_proj = self.text_projector(text_features) + self.text_pos_embed\n        updated_image_features, updated_text_features = image_features_proj, text_features_proj\n        for img_layer, txt_layer in zip(self.image_fusion_layers, self.text_fusion_layers):\n            temp_img = img_layer(tgt=updated_image_features, memory=updated_text_features, memory_key_padding_mask=text_inputs.attention_mask == 0)\n            temp_txt = txt_layer(tgt=updated_text_features, memory=updated_image_features)\n            updated_image_features, updated_text_features = temp_img, temp_txt\n        return torch.cat([updated_image_features, updated_text_features], dim=1), text_inputs.attention_mask == 0\n\nclass Stage2_ObjectReasoner(nn.Module):\n    def __init__(self, hidden_dim: int = 256, num_queries: int = 10):\n        super().__init__()\n        self.num_queries = num_queries\n        self.object_queries = nn.Parameter(torch.randn(1, num_queries, hidden_dim))\n        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=8, dim_feedforward=hidden_dim * 4, batch_first=True)\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n\n    def forward(self, fused_tokens: torch.Tensor, fused_tokens_padding_mask: torch.Tensor):\n        queries = self.object_queries.repeat(fused_tokens.shape[0], 1, 1)\n        return self.decoder(tgt=queries, memory=fused_tokens, memory_key_padding_mask=fused_tokens_padding_mask)\n\nclass HiRes_Core_Model(nn.Module):\n    def __init__(self, image_size=224, patch_size=16):\n        super().__init__()\n        self.num_image_patches = (image_size // patch_size) ** 2\n        self.stage1 = Stage1_FusionModule()\n        self.num_text_tokens = self.stage1.text_encoder.config.max_position_embeddings\n        self.stage2 = Stage2_ObjectReasoner(hidden_dim=self.stage1.hidden_dim)\n\n    def forward(self, images: torch.Tensor, texts: list[str]):\n        fused_tokens, text_padding_mask = self.stage1(images, texts)\n        image_padding_mask = torch.zeros(fused_tokens.shape[0], self.num_image_patches, dtype=torch.bool, device=fused_tokens.device)\n        full_padding_mask = torch.cat([image_padding_mask, text_padding_mask], dim=1)\n        return self.stage2(fused_tokens, full_padding_mask)\n\nclass ViTFeatureExtractor(nn.Module):\n    def __init__(self, vit_model_name='vit_base_patch16_224_in21k', feature_indices=(2, 5, 8, 11)):\n        super().__init__()\n        self.vit = timm.create_model(vit_model_name, pretrained=True)\n        self.feature_indices = feature_indices\n        self.patch_size = self.vit.patch_embed.patch_size[0]\n\n    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n        B, C, H, W = x.shape; H_patch, W_patch = H // self.patch_size, W // self.patch_size\n        x = self.vit.patch_embed(x)\n        x = torch.cat((self.vit.cls_token.expand(B, -1, -1), x), dim=1)\n        x = self.vit.pos_drop(x + self.vit.pos_embed)\n        features = {}\n        for i, blk in enumerate(self.vit.blocks):\n            x = blk(x)\n            if i in self.feature_indices:\n                feature_map = x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, H_patch, W_patch)\n                features[f\"scale_{i}\"] = feature_map\n        return features\n\nclass UpsampleBlock(nn.Module):\n    def __init__(self, dim: int):\n        super().__init__()\n        # Bilinear upsample + convs (empirically sharper than single ConvTranspose2d step)\n        self.conv = nn.Sequential(\n            nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(dim),\n            nn.GELU(),\n            nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(dim),\n            nn.GELU(),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=False)\n        return self.conv(x)\n\nclass PixelDecoderHighRes(nn.Module):\n    \"\"\"\n    Progressive, learnable upsampling all the way to image_size.\n    Takes multi-scale ViT features (all at patch resolution) -> projects -> fuses -> upsample×k to target HxW.\n    \"\"\"\n    def __init__(self, input_dims: Dict[str, int], output_dim: int = 256, image_size: int = 224, vit_patch: int = 16):\n        super().__init__()\n        self.output_dim = output_dim\n        self.image_size = image_size\n        self.patch = vit_patch\n\n        # 1×1 lateral projections to a common dim\n        self.input_proj = nn.ModuleDict({\n            name: nn.Conv2d(in_dim, output_dim, kernel_size=1)\n            for name, in_dim in input_dims.items()\n        })\n\n        # Small fusion head after summation\n        self.fuse = nn.Sequential(\n            nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(output_dim),\n            nn.GELU()\n        )\n\n        # Build enough 2× upsample steps: (image_size/patch) is patch grid size (e.g., 14 for 224/16)\n        # We need to go from 14 -> 224, i.e., 4 doublings: 14→28→56→112→224\n        grid_size = image_size // vit_patch\n        assert image_size % vit_patch == 0, \"image_size must be divisible by vit_patch\"\n        # Number of 2× steps to reach target size\n        steps = int(np.round(np.log2(image_size / grid_size)))\n        self.ups = nn.ModuleList([UpsampleBlock(output_dim) for _ in range(steps)])\n\n        # Light refinement at full res\n        self.refine_full = nn.Sequential(\n            nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(output_dim),\n            nn.GELU()\n        )\n\n    def forward(self, features: Dict[str, torch.Tensor]) -> torch.Tensor:\n        # Project all scales to common channel dim and sum at the patch grid resolution\n        # (With ViT-patch16, all keys are same spatial size already; we still robustly upsample if not.)\n        # Sort keys to have a deterministic order\n        keys = sorted(features.keys(), key=lambda k: int(k.split(\"_\")[-1]))\n        proj = []\n        target_hw = None\n        for k in keys:\n            x = features[k]                      # [B, Ck, Hp, Wp]\n            x = self.input_proj[k](x)            # [B, D,  Hp, Wp]\n            if target_hw is None:\n                target_hw = x.shape[-2:]\n                proj.append(x)\n            else:\n                proj.append(F.interpolate(x, size=target_hw, mode=\"bilinear\", align_corners=False))\n        fused = torch.stack(proj, dim=0).sum(0)   # [B, D, Hp, Wp]\n        fused = self.fuse(fused)\n\n        # Progressive learned upsampling to full resolution\n        y = fused\n        for up in self.ups:\n            y = up(y)\n\n        # Final refinement at full resolution\n        y = self.refine_full(y)                  # [B, D, H=img_size, W=img_size]\n        return y\n\n\nclass Stage4_Mask2FormerDecoder(nn.Module):\n    def __init__(self, hidden_dim: int = 256, num_queries: int = 10):\n        super().__init__()\n        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=8, dim_feedforward=hidden_dim*4, batch_first=True)\n        self.query_refiner = nn.TransformerDecoder(decoder_layer, num_layers=2)\n        self.mask_embed_head = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n\n    def forward(self, object_tokens: torch.Tensor, pixel_embeddings: torch.Tensor) -> torch.Tensor:\n        B, C, H, W = pixel_embeddings.shape\n        pixel_embeddings_flat = pixel_embeddings.flatten(2).permute(0, 2, 1)\n        refined_tokens = self.query_refiner(tgt=object_tokens, memory=pixel_embeddings_flat)\n        mask_embeddings = self.mask_embed_head(refined_tokens)\n        mask_logits = (mask_embeddings @ pixel_embeddings.flatten(2)) / np.sqrt(pixel_embeddings.shape[1])\n        return mask_logits.view(B, -1, H, W)\n\nclass HiRes_Full_Model(nn.Module):\n    def __init__(self, image_size=224, patch_size=16, hidden_dim=256, num_queries=1):\n        super().__init__()\n        self.image_size = image_size\n        self.patch_size = patch_size\n\n        self.feature_extractor = ViTFeatureExtractor(vit_model_name='vit_base_patch16_224_in21k', feature_indices=(2,5,8,11))\n        self.reasoning_core = HiRes_Core_Model(image_size=image_size, patch_size=patch_size)\n\n        feature_dims = {f\"scale_{i}\": 768 for i in self.feature_extractor.feature_indices}\n        self.pixel_decoder = PixelDecoderHighRes(\n            input_dims=feature_dims,\n            output_dim=hidden_dim,\n            image_size=image_size,\n            vit_patch=self.feature_extractor.patch_size\n        )\n        self.mask_decoder = Stage4_Mask2FormerDecoder(hidden_dim=hidden_dim, num_queries=num_queries)\n        \n    def forward(self, images: torch.Tensor, texts: List[str]) -> Dict[str, torch.Tensor]:\n        # 1) Multi-scale ViT features (patch grid)\n        multi_scale_features = self.feature_extractor(images)                     # {scale_i: [B, 768, Hp, Wp]}\n        # 2) High-res pixel embeddings (full 224×224)\n        pixel_embeddings = self.pixel_decoder(multi_scale_features)               # [B, D, H, W]\n        # 3) Object queries from the reasoning core\n        object_tokens = self.reasoning_core(images, texts)                        # [B, Q, D]\n        # 4) Predict masks directly at full resolution (no final interpolate anymore)\n        predicted_masks = self.mask_decoder(object_tokens, pixel_embeddings)      # [B, Q, H, W]\n        return {\"pred_masks\": predicted_masks}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:09.888486Z","iopub.execute_input":"2025-09-09T16:39:09.888669Z","iopub.status.idle":"2025-09-09T16:39:09.921252Z","shell.execute_reply.started":"2025-09-09T16:39:09.888655Z","shell.execute_reply":"2025-09-09T16:39:09.920449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- Freezing helpers ----------\ndef freeze_backbone_and_text(model: HiRes_Full_Model, freeze_vit=True, freeze_clip_text=True):\n    \"\"\"\n    Freezes pretrained backbones in HiRes_Full_Model:\n      - ViT feature extractor (multi-scale)\n      - ViT encoder inside Stage1\n      - CLIP text encoder inside Stage1\n    \"\"\"\n    # Freeze ViT feature extractor (multi-scale)\n    if freeze_vit:\n        try:\n            model.feature_extractor.vit.requires_grad_(False)\n            print(\"Froze ViT feature_extractor.\")\n        except Exception as ex:\n            print(\"Could not freeze feature_extractor ViT:\", ex)\n\n        try:\n            model.reasoning_core.stage1.vit_encoder.requires_grad_(False)\n            print(\"Froze Stage1 ViT encoder.\")\n        except Exception as ex:\n            print(\"Could not freeze Stage1 ViT:\", ex)\n\n    # Freeze CLIP text encoder\n    if freeze_clip_text:\n        try:\n            model.reasoning_core.stage1.text_encoder.requires_grad_(False)\n            print(\"Froze CLIP text encoder.\")\n        except Exception as ex:\n            print(\"Could not freeze CLIP text encoder:\", ex)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:09.922099Z","iopub.execute_input":"2025-09-09T16:39:09.922290Z","iopub.status.idle":"2025-09-09T16:39:09.943685Z","shell.execute_reply.started":"2025-09-09T16:39:09.922274Z","shell.execute_reply":"2025-09-09T16:39:09.942837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- Matching + Loss (Hungarian) ----------\ndef sigmoid_flat(x):\n    return torch.sigmoid(x).flatten(start_dim=1)  # [N, HW]\n\ndef compute_pairwise_cost(pred_logits_q_hw: torch.Tensor, gt_mask_hw: torch.Tensor):\n    \"\"\"\n    pred_logits_q_hw: (Q, HW) logits (torch)\n    gt_mask_hw: (G, HW) 0/1 targets (torch)\n    Returns cost matrix (Q x G) of floats.\n    \"\"\"\n    Q, HW = pred_logits_q_hw.shape\n    G = gt_mask_hw.shape[0]\n    cost = torch.zeros((Q, G), device=pred_logits_q_hw.device)\n    # BCE per pair\n    for i in range(G):\n        tgt = gt_mask_hw[i].unsqueeze(0).expand(Q, -1)  # [Q,HW]\n        bce = F.binary_cross_entropy_with_logits(pred_logits_q_hw, tgt, reduction='none').mean(dim=1)  # [Q]\n        # IoU (on probs)\n        pred_prob = torch.sigmoid(pred_logits_q_hw)\n        inter = (pred_prob * tgt).sum(dim=1)\n        union = (pred_prob + tgt - pred_prob * tgt).sum(dim=1) + 1e-6\n        iou = inter / union\n        # combine (lower cost = better match)\n        cost[:, i] = bce - 0.8 * iou  # weights: adjust as needed\n    return cost.cpu().detach().numpy()  # to feed linear_sum_assignment\n\ndef hungarian_loss_for_sample(pred_logits_q_hw: torch.Tensor, gt_masks_g_hw: torch.Tensor, no_object_cost=0.2):\n    \"\"\"\n    pred_logits_q_hw: [Q, HW] logits\n    gt_masks_g_hw: [G, HW] 0/1\n    Returns loss scalar for this sample.\n    \"\"\"\n    Q, HW = pred_logits_q_hw.shape\n    G = gt_masks_g_hw.shape[0]\n    device = pred_logits_q_hw.device\n    if G == 0:\n        # No GT masks: encourage all queries to predict background (zeros)\n        loss_noobj = F.binary_cross_entropy_with_logits(pred_logits_q_hw, torch.zeros_like(pred_logits_q_hw), reduction='mean')\n        return loss_noobj\n    # compute cost matrix\n    cost = compute_pairwise_cost(pred_logits_q_hw, gt_masks_g_hw)  # Q x G numpy\n    row_ind, col_ind = linear_sum_assignment(cost)\n    # keep only up to G matches: but linear_sum_assignment will produce min(Q,G) matches\n    matched_q = torch.tensor(row_ind, dtype=torch.long, device=device)\n    matched_g = torch.tensor(col_ind, dtype=torch.long, device=device)\n    # matched losses\n    matched_loss = 0.0\n    for mq, mg in zip(matched_q.tolist(), matched_g.tolist()):\n        tgt = gt_masks_g_hw[mg].unsqueeze(0)  # [1, HW]\n        pred = pred_logits_q_hw[mq].unsqueeze(0)  # [1, HW]\n        bce = F.binary_cross_entropy_with_logits(pred, tgt, reduction='mean')\n        # dice loss to complement BCE\n        p = torch.sigmoid(pred)\n        inter = (p * tgt).sum()\n        union = p.sum() + tgt.sum()\n        dice = 1 - (2 * inter + 1e-6) / (union + 1e-6)\n        matched_loss = matched_loss + (bce + dice)\n    matched_loss = matched_loss / max(1, len(matched_q))\n\n    # no-object loss for unmatched queries\n    matched_mask = torch.zeros(Q, dtype=torch.bool, device=device)\n    matched_mask[matched_q] = True\n    if matched_mask.sum() < Q:\n        unmatched_idxs = (~matched_mask).nonzero(as_tuple=False).squeeze(1)\n        noobj_preds = pred_logits_q_hw[unmatched_idxs]\n        # encourage background: target zeros\n        noobj_loss = F.binary_cross_entropy_with_logits(noobj_preds, torch.zeros_like(noobj_preds), reduction='mean')\n    else:\n        noobj_loss = torch.tensor(0.0, device=device)\n\n    return matched_loss + 0.5 * noobj_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:09.944569Z","iopub.execute_input":"2025-09-09T16:39:09.944829Z","iopub.status.idle":"2025-09-09T16:39:09.966498Z","shell.execute_reply.started":"2025-09-09T16:39:09.944801Z","shell.execute_reply":"2025-09-09T16:39:09.965692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\n\nimage_root = None  # set to your COCO images folder if needed, e.g. \"/data/coco/images/val2017\"\n\n\ntrain_split = grefcoco[\"train\"]\nval_split = grefcoco[\"train\"] \n\ntrain_ds = GRefCocoTorchDataset(train_split, image_size=224, train=True)\nval_ds = GRefCocoTorchDataset(val_split, image_size=224, train=False)\n\ntrain_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=grefcoco_collate_fn, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=grefcoco_collate_fn, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:09.968867Z","iopub.execute_input":"2025-09-09T16:39:09.969274Z","iopub.status.idle":"2025-09-09T16:39:09.987671Z","shell.execute_reply.started":"2025-09-09T16:39:09.969255Z","shell.execute_reply":"2025-09-09T16:39:09.987002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_random_sample(dataset, idx=None, alpha=0.5):\n    \"\"\"\n    Visualize a random sample from the dataset.\n    Shows: image, mask (combined if multiple), and image+mask overlay.\n    \n    Args:\n        dataset: RefCocoTorchDataset instance\n        idx: optional index. If None, a random index is chosen.\n        alpha: transparency for overlay\n    \"\"\"\n    if idx is None:\n        idx = random.randint(0, len(dataset) - 1)\n\n    sample = dataset[idx]\n    img_t = sample[\"image\"]  # [3,H,W] normalized\n    masks = sample[\"gt_masks\"]  # [G,H,W]\n    txt = sample[\"text\"]\n\n    # Denormalize image (ImageNet mean/std)\n    mean = np.array([0.485, 0.456, 0.406])\n    std  = np.array([0.229, 0.224, 0.225])\n    img_np = img_t.permute(1, 2, 0).cpu().numpy()\n    img_np = (img_np * std + mean).clip(0, 1)\n\n    # Combine all masks into one (for visualization simplicity)\n    if masks.numel() > 0:\n        mask_np = masks.max(dim=0)[0].cpu().numpy()  # [H,W], values 0/1\n    else:\n        mask_np = np.zeros((img_np.shape[0], img_np.shape[1]))\n\n    # Create overlay\n    overlay = img_np.copy()\n    overlay[mask_np > 0.5, :] = (1 - alpha) * overlay[mask_np > 0.5, :] + alpha * np.array([1, 0, 0])  # red mask\n\n    # Plot\n    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n    axs[0].imshow(img_np)\n    axs[0].set_title(\"Image\")\n    axs[0].axis(\"off\")\n\n    axs[1].imshow(mask_np, cmap=\"gray\")\n    axs[1].set_title(\"Mask\")\n    axs[1].axis(\"off\")\n\n    axs[2].imshow(overlay)\n    axs[2].set_title(\"Image + Mask\")\n    axs[2].axis(\"off\")\n\n    fig.suptitle(f\"Sample {idx} | Text: {txt}\", fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\n\nvisualize_random_sample(train_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:09.988424Z","iopub.execute_input":"2025-09-09T16:39:09.988679Z","iopub.status.idle":"2025-09-09T16:39:10.352116Z","shell.execute_reply.started":"2025-09-09T16:39:09.988655Z","shell.execute_reply":"2025-09-09T16:39:10.351214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Initialize model\nmodel = HiRes_Full_Model(image_size=224, patch_size=16, hidden_dim=256, num_queries=10)\n\n# Move to GPU\nmodel = model.to(device)\n\n# Freeze parts as before\nfreeze_backbone_and_text(model, freeze_vit=True, freeze_clip_text=True)\n\n# Optimizer only sees trainable params\noptimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4, weight_decay=1e-2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:10.353008Z","iopub.execute_input":"2025-09-09T16:39:10.353291Z","iopub.status.idle":"2025-09-09T16:39:14.753536Z","shell.execute_reply.started":"2025-09-09T16:39:10.353270Z","shell.execute_reply":"2025-09-09T16:39:14.752817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_params = sum(p.numel() for p in model.parameters())\nnum_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total params: {num_params:,}, Trainable: {num_trainable:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:14.754331Z","iopub.execute_input":"2025-09-09T16:39:14.754521Z","iopub.status.idle":"2025-09-09T16:39:14.763008Z","shell.execute_reply.started":"2025-09-09T16:39:14.754505Z","shell.execute_reply":"2025-09-09T16:39:14.762210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom tqdm import tqdm\n\ndef train(rank, world_size, train_dataset):\n    # Setup DDP\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\n    # Create model + move to correct GPU\n    model = HiRes_Full_Model(image_size=224, patch_size=16, hidden_dim=256, num_queries=10)\n    model = model.to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n    optimizer = torch.optim.AdamW(ddp_model.parameters(), lr=1e-4)\n\n    # Distributed sampler for balanced data splits\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset, num_replicas=world_size, rank=rank, shuffle=True\n    )\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=8, sampler=train_sampler,\n        num_workers=4, pin_memory=True\n    )\n\n    num_epochs = 1\n    for epoch in range(num_epochs):\n        train_sampler.set_epoch(epoch)\n        ddp_model.train()\n        epoch_loss = 0.0\n\n        if rank == 0:\n            loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        else:\n            loop = train_loader  # no tqdm on other GPUs\n\n        for images, texts, gt_masks_list in loop:\n            images = images.to(rank, non_blocking=True)     # [B,3,H,W]\n            texts = texts.to(rank, non_blocking=True)       # e.g., tokenized CLIP text\n            # forward\n            out = ddp_model(images, texts)  # {\"pred_masks\": [B, Q, H, W]}\n            pred_masks = out[\"pred_masks\"]  # already on GPU(rank)\n\n            B, Q, H, W = pred_masks.shape\n            total_loss = torch.tensor(0.0, device=rank)\n\n            for b in range(B):\n                pred_logits_q_hw = pred_masks[b].view(Q, -1)  # [Q, HW]\n                gt_masks = gt_masks_list[b].to(rank)\n                if gt_masks.shape[0] == 0:\n                    loss_b = hungarian_loss_for_sample(pred_logits_q_hw,\n                                torch.zeros((0, H*W), device=rank))\n                else:\n                    gt_flat = gt_masks.view(gt_masks.shape[0], -1)  # [G, HW]\n                    loss_b = hungarian_loss_for_sample(pred_logits_q_hw, gt_flat)\n                total_loss = total_loss + loss_b\n\n            total_loss = total_loss / B\n\n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n\n            epoch_loss += total_loss.item()\n            if rank == 0:  # only main GPU prints\n                loop.set_postfix(loss=total_loss.item())\n\n        if rank == 0:\n            print(f\"Epoch {epoch+1} avg loss: {epoch_loss / len(train_loader):.4f}\")\n\n    dist.destroy_process_group()\n\ndef run_training(train_dataset):\n    world_size = torch.cuda.device_count()  # Kaggle usually = 2\n    mp.spawn(train, args=(world_size, train_dataset), nprocs=world_size, join=True)\n\nif __name__ == \"__main__\":\n    mp.set_start_method(\"fork\", force=True)\n    run_training(train_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:53:01.891618Z","iopub.execute_input":"2025-09-09T16:53:01.891909Z","iopub.status.idle":"2025-09-09T16:53:04.001418Z","shell.execute_reply.started":"2025-09-09T16:53:01.891890Z","shell.execute_reply":"2025-09-09T16:53:04.000369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- Training loop with Hungarian loss ----------\nnum_epochs = 1\nmodel.train()\nfor epoch in range(num_epochs):\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    epoch_loss = 0.0\n    for images, texts, gt_masks_list in loop:\n        images = images.to(device)  # [B,3,H,W]\n        # forward\n        out = model(images, texts)  # {\"pred_masks\": [B, Q, H, W] logits}\n        pred_masks = out[\"pred_masks\"].to(device)  # [B, Q, H, W]\n        B, Q, H, W = pred_masks.shape\n        total_loss = torch.tensor(0.0, device=device)\n        for b in range(B):\n            pred_logits_q_hw = pred_masks[b].view(Q, -1)  # [Q, HW]\n            gt_masks = gt_masks_list[b].to(device)  # [G, H, W] or shape (0, H, W)\n            if gt_masks.shape[0] == 0:\n                # no GT masks\n                loss_b = hungarian_loss_for_sample(pred_logits_q_hw, torch.zeros((0, H*W), device=device))\n            else:\n                gt_flat = gt_masks.view(gt_masks.shape[0], -1)  # [G, HW]\n                loss_b = hungarian_loss_for_sample(pred_logits_q_hw, gt_flat)\n            total_loss = total_loss + loss_b\n        total_loss = total_loss / B\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n        epoch_loss += total_loss.item()\n        loop.set_postfix(loss=total_loss.item())\n    print(f\"Epoch {epoch+1} avg loss: {epoch_loss / len(train_loader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:14.803727Z","iopub.status.idle":"2025-09-09T16:39:14.804106Z","shell.execute_reply.started":"2025-09-09T16:39:14.803900Z","shell.execute_reply":"2025-09-09T16:39:14.803917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- Simple inference & visualization ----------\nmodel.eval()\ndef denorm_image(tensor_img):\n    mean = torch.tensor([0.485, 0.456, 0.406], device=tensor_img.device).view(3,1,1)\n    std = torch.tensor([0.229, 0.224, 0.225], device=tensor_img.device).view(3,1,1)\n    img = tensor_img * std + mean\n    img = img.clamp(0,1).cpu().permute(1,2,0).numpy()\n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:14.805727Z","iopub.status.idle":"2025-09-09T16:39:14.806066Z","shell.execute_reply.started":"2025-09-09T16:39:14.805867Z","shell.execute_reply":"2025-09-09T16:39:14.805884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    for images, texts, gt_masks_list in val_loader:\n        images = images.to(device)\n        out = model(images, texts)\n        preds = out[\"pred_masks\"]  # [B, Q, H, W]\n        B, Q, H, W = preds.shape\n        for b in range(min(2, B)):\n            # collapse best query per GT (or max)\n            pred_logits = preds[b]  # [Q,H,W]\n            pred_best = torch.sigmoid(pred_logits).max(dim=0).values.cpu().numpy()  # [H,W]\n            img_np = denorm_image(images[b])\n            gt_mask = gt_masks_list[b]\n            if isinstance(gt_mask, torch.Tensor) and gt_mask.shape[0] > 0:\n                gt_overlay = gt_mask[0].numpy()\n            else:\n                gt_overlay = np.zeros((H, W))\n            fig, axs = plt.subplots(1,3,figsize=(12,4))\n            axs[0].imshow(img_np); axs[0].axis('off'); axs[0].set_title(\"Image\")\n            axs[1].imshow(img_np); axs[1].imshow(gt_overlay, alpha=1, cmap='Reds'); axs[1].axis('off'); axs[1].set_title(\"GT\")\n            axs[2].imshow(img_np); axs[2].imshow(pred_best, alpha=1, cmap='Blues'); axs[2].axis('off'); axs[2].set_title(\"Pred\")\n            plt.show()\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:39:14.807342Z","iopub.status.idle":"2025-09-09T16:39:14.807548Z","shell.execute_reply.started":"2025-09-09T16:39:14.807451Z","shell.execute_reply":"2025-09-09T16:39:14.807459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}